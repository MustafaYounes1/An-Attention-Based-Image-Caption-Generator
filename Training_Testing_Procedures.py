from Data_PreProcessing import *
from Encoder import *
from Decoder import *
from time import sleep

os.chdir(os.path.dirname(__file__))

BATCH_SIZE = 64
BUFFER_SIZE = 1000
embedding_dim = 256
units = 512
vocab_size = 10000 + 1  # (vocab_size after tokenization process) + 1 is because of reserving padding (i.e. index zero).
train_num_steps = len(load_train_images_paths_and_train_captions()[0]) // BATCH_SIZE  # 1984 batches for training phase.
test_num_steps = len(load_test_images_paths_and_test_captions()[0]) // BATCH_SIZE  # 496 batches for testing phase.
# Shape of the vector extracted from EfficientB7 is (324, 2560)
# These two variables represent that vector shape
features_shape = 2560
attention_features_shape = 324
tokenizer = load_tokenizer()

# train_dataset = create_training_dataset(batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE)
# test_dataset = create_testing_dataset(batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE)

encoder = EfficientNetB7_Encoder(embedding_dim)
decoder = RNN_Decoder(embedding_dim, units, vocab_size)

optimizer = tf.keras.optimizers.Adam()  # Optimizer that implements the Adam algorithm.

""" ADAM Optimizer      - adaptive moment estimation  

Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and
second-order moments.
Adam combines the best properties of the AdaGrad (Adaptive Gradient) and RMSProp (Root Mean Square) algorithms to 
provide an optimization algorithm that can handle sparse gradients on noisy problems.

"""

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')  # Computes the crossentropy loss between the labels and predictions.
"""
SparseCategoricalCrossentropy and CategoricalCrossentropy both compute categorical cross-entropy. 
The only difference is in how the targets/labels should be encoded.

Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as 
integers. If you want to provide labels using one-hot representation, use CategoricalCrossentropy loss.

- from_logits	    Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a 
                    probability distribution. **Note - Using from_logits=True may be more numerically stable.
                    
In ML, logits mean the vector of raw (non-normalized) predictions that a classification model generates, which is 
ordinarily then passed to a normalization function. If the model is solving a multi-class classification problem, 
logits typically become an input to the softmax function. The softmax function then generates a vector of (normalized) 
probabilities with one value for each possible class.
The from_logits=True attribute inform the loss function that the output values generated by the model are not 
normalized, a.k.a. logits. In other words, the softmax function has not been applied on them to produce a probability 
distribution. Therefore, the output layer in this case does not have a softmax activation function
                    
- reduction	        (Optional) Type of tf.keras.losses.Reduction to apply to loss.
"""


# tf.math.logical_not(x)            - Returns the truth value of NOT x (element-wise)(element must be booleans).
# tf.math.equal(x, y)               - Returns the truth value of (x == y) (element-wise).
# tf.cast(x, dtype, name=None)      - Casts a tensor to a new type.
# Next function will be looking into indices of the real values where you have a 0 and using multiplication later on to
# make their loss contribution 0. (0 values after tokenization process means padding)
def loss_function(real, pred):  # calculate the loss per batch for one time step
    mask = tf.math.logical_not(tf.math.equal(real, 0))  # Find all 0 in y_true and mark them as False
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask  # Add the weight of the mask to the normally calculated loss to remove the effect of padding 0

    return tf.reduce_mean(loss_)  # Finally average the loss


# TensorFlow objects provide an easy automatic mechanism for saving and restoring the values of variables they use.
checkpoint_path = 'Checkpoints/Train'

"""
* tf.train.Checkpoint(root=None, **kwargs)  - Manages saving/restoring trackable values to disk.

TensorFlow objects may contain trackable state, such as tf.Variables, tf.keras.optimizers.Optimizer implementations, 
tf.data.Dataset iterators, tf.keras.Layer implementations, or tf.keras.Model implementations. These are called 
trackable objects.
A Checkpoint object can be constructed to save either a single or group of trackable objects to a checkpoint file. 
It maintains a save_counter for numbering checkpoints.

- root  	The root object to checkpoint.
- **kwargs	Keyword arguments are set as attributes of this object, and are saved with the checkpoint. Values must be 
            trackable objects.
            
Attributes: save_counter, Incremented when save() is called. Used to number checkpoints.

Methods: restore(save_path, options=None)

Restores a training checkpoint.
Restores this Checkpoint and any objects it depends on.
This method is intended to be used to load checkpoints created by save(). For checkpoints created by write() use the 
read() method which does not expect the save_counter variable added by save().
"""
ckpt = tf.train.Checkpoint(encoder=encoder,
                           decoder=decoder,
                           optimizer=optimizer)

"""
* tf.train.CheckpointManager(
    checkpoint, directory, max_to_keep, keep_checkpoint_every_n_hours=None,
    checkpoint_name='ckpt', step_counter=None, checkpoint_interval=None,
    init_fn=None
)       
Manages multiple checkpoints by keeping some and deleting unneeded ones.
CheckpointManager preserves its own state across instantiations.  Only one should be active in a particular directory 
at a time.

- checkpoint	    The tf.train.Checkpoint instance to save and manage checkpoints for.

- directory	        The path to a directory in which to write checkpoints. A special file named "checkpoint" is also 
                    written to this directory (in a human-readable text format) which contains the state of the 
                    CheckpointManager.
                    
- max_to_keep	    An integer, the number of checkpoints to keep. Unless preserved by keep_checkpoint_every_n_hours, 
                    checkpoints will be deleted from the active set, oldest first, until only max_to_keep checkpoints 
                    remain. If None, no checkpoints are deleted and everything stays in the active set. 
                    Note that max_to_keep=None will keep all checkpoint paths in memory and in the checkpoint state 
                    protocol buffer on disk.
                    
Methods:
    - save(): Creates a new checkpoint and manages it.
    
Attributes:
    - latest_checkpoint: 	The prefix of the most recent checkpoint in directory.
                            Equivalent to tf.train.latest_checkpoint(directory) where directory is the constructor 
                            argument to CheckpointManager.
                            Suitable for passing to tf.train.Checkpoint.restore to resume training.
"""
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=20)

""" Training Procedure

- The ENCODER output, hidden state(initialised to 0) and the DECODER input(which is the < start > token) are passed to 
  the DECODER.
  
- The DECODER returns the predictions and the DECODER last hidden state and the attention weights.

- The DECODER hidden state is then passed back into the model and the predictions are used to calculate the loss. While 
  training, we use the Teacher Forcing technique, to decide the next input of the DECODER.

- Teacher Forcing is the technique where the target word is passed as the next input to the DECODER. 

- Final step is to calculate the Gradient and apply it to the optimizer and backpropagate

"""

""" Teacher forcing for Recurrent Neural Networks

* Teacher forcing is a method for quickly and efficiently training recurrent neural network models that use the ground 
  truth from a prior time step as input, instead of model output from a prior time step as an input.
* using the model output from the prior time step as input makes learning slower and the model unstable.

"""

""" Decorator

A decorator is a design pattern in Python that allows a user to add new functionality to an existing object without 
modifying its structure. Decorators are usually called before the definition of a function you want to decorate.

"""

""" tf.function     - Compiles a function into a callable TensorFlow graph.

tf.function constructs a callable that executes a TensorFlow graph (tf.Graph) created by trace-compiling the TensorFlow 
operations in func, effectively executing func as a TensorFlow graph.
tf. function is a decorator function provided by Tensorflow 2.0 that converts regular python code to a callable 
Tensorflow graph function, which is usually more performant and python independent

"""


@tf.function
def train_step(img_tensor, target):  # this function represent each training step (each training batch)
    loss = 0

    # initializing the hidden state for each batch, because the captions are not related from image to image.
    hidden = decoder.reset_state(batch_size=target.shape[0])  # BATCH_SIZE is the same as target.shape[0]

    # we should initialize the input for the decoder to be equal to the index of <start> token
    # So, here we will prepare a tensor of shape (64, 1) for each batch to be the initial input of the DECODER.
    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)

    """ tf.GradientTape     - Record operations for automatic differentiation.
    
    To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the 
    forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to 
    compute gradients.
    
    """
    with tf.GradientTape() as tape:
        features = encoder(img_tensor)  # computing the EfficientB7_Encoder outputs only once per batch.

        # Since we will feed the decoder with initialised tensor of <start> token, we will walk through the tokenized
        # captions from index 1 (excluding <start> token) till the end of the caption.
        for i in range(1, target.shape[1]):
            # passing the inputs through the decoder (inputs_for_this_time_step, features, last_hidden_State)
            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)

            loss += loss_function(target[:, i], predictions)

            # using Teacher Forcing
            dec_input = tf.expand_dims(target[:, i], 1)  # dec_input shape will be (batch_size, 1) representing the
            # target word for the time step i for each caption in the batch

    total_loss = (loss / int(target.shape[1]))  # for estimation purposes (loss plotting).

    trainable_variables = encoder.trainable_variables + decoder.trainable_variables

    gradients = tape.gradient(loss, trainable_variables)  # Computes the gradient using operations recorded in context
    # of this tape.

    optimizer.apply_gradients(zip(gradients, trainable_variables))  # Apply gradients to variables.

    return loss, total_loss


@tf.function
def test_step(img_tensor, target):  # this function represent each testing step (each testing batch)
    loss = 0
    hidden = decoder.reset_state(batch_size=target.shape[0])
    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)
    features = encoder(img_tensor)

    for i in range(1, target.shape[1]):
        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)
        loss += loss_function(target[:, i], predictions)
        # tf.argmax     - Returns the index with the largest value across axes of a tensor ( 0 --> cols, 1 --> rows).
        predicted_id = tf.argmax(predictions, 1)  # shape = (64, )
        dec_input = tf.expand_dims(predicted_id, 1)  # shape = (64, 1)

    total_loss = (loss / int(target.shape[1]))

    return loss, total_loss


# Initializing two lists (if not exist) to sore train and test losses while training.
if 'train_loss_plot.pkl' not in os.listdir('Loss Plots') and 'test_loss_plot.pkl' not in os.listdir('Loss Plots'):
    train_loss_plot_initial, test_loss_plot_initial = [], []
    dump(train_loss_plot_initial, open('Loss Plots/train_loss_plot.pkl', 'wb'))
    dump(test_loss_plot_initial, open('Loss Plots/test_loss_plot.pkl', 'wb'))


def calculate_time(start_time):
    elapsed_time = time.time() - start_time
    days = 0
    if elapsed_time >= 86400:
        days = int(elapsed_time / 86400)
    elapsed = time.strftime("%H:%M:%S", time.gmtime(time.time() - start_time))
    return days, elapsed


def start_training():
    start_epoch = 0  # initially we will start from epoch 0
    beginning_time = time.time()

    if ckpt_manager.latest_checkpoint:  # Resume training from where you left off, and restore the last checkpoint,
        start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])
        # restoring the latest checkpoint in checkpoint_path
        ckpt.restore(ckpt_manager.latest_checkpoint)

    for epoch in range(start_epoch, 20):

        start = time.time()

        train_loss_plot = load(open('Loss Plots/train_loss_plot.pkl', 'rb'))
        test_loss_plot = load(open('Loss Plots/test_loss_plot.pkl', 'rb'))

        # For Train
        # ================================================================
        total_loss_train = 0
        for (batch, (features_batch, captions_batch)) in enumerate(tqdm(train_dataset, desc=f'Epoch {epoch+1} - Training', unit='batch')):
            batch_loss, t_loss = train_step(features_batch, captions_batch)
            total_loss_train += t_loss
        # storing the epoch end loss value to plot later
        train_loss_plot.append(total_loss_train / train_num_steps)
        dump(train_loss_plot, open('Loss Plots/train_loss_plot.pkl', 'wb'))

        sleep(5)

        # For Test
        # ================================================================
        total_loss_test = 0
        for (batch, (features_batch, captions_batch)) in enumerate(tqdm(test_dataset, desc=f'Epoch {epoch+1} - Testing', unit='batch')):
            batch_loss, t_loss = test_step(features_batch, captions_batch)
            total_loss_test += t_loss
        # storing the epoch end loss value to plot later
        test_loss_plot.append(total_loss_test / test_num_steps)
        dump(test_loss_plot, open('Loss Plots/test_loss_plot.pkl', 'wb'))

        ckpt_manager.save()  # Creating a checkpoint.

        print('Epoch {}: TrainLoss {:.6f} , TestLoss {:.6f}'.format(epoch + 1, (total_loss_train / train_num_steps),
                                                                    (total_loss_test / test_num_steps)))
        days, elapsed_time = calculate_time(start)
        if days:
            print('Time taken for this epoch: {} Day(s) and {} \n'.format(days, elapsed_time))
        else:
            print('Time taken for this epoch: {} \n'.format(elapsed_time))

        sleep(5)

    # Saving the Encoder and Decoder Models with SavedModel format (Models saved in this format can be restored
    # using tf.keras.models.load_model and are compatible with TensorFlow Serving. The SavedModel format is a
    # directory containing a protobuf binary and a TensorFlow checkpoint.)
    encoder.save('Encoder model', save_format='tf')
    decoder.save('Decoder model', save_format='tf')

    days, elapsed_time = calculate_time(beginning_time)
    if days:
        print(f"Training and Testing Procedures took: {days} day(s) and {elapsed_time}")
    else:
        print(f"Training and Testing Procedures took: {elapsed_time}")


def plot_losses():
    train_loss_plot = load(open('Loss Plots/train_loss_plot.pkl', 'rb'))
    test_loss_plot = load(open('Loss Plots/test_loss_plot.pkl', 'rb'))
    train_loss_plot = [float(x) for x in train_loss_plot]
    test_loss_plot = [float(x) for x in test_loss_plot]
    for loss in [train_loss_plot, test_loss_plot]:
        plt.plot(loss)
    plt.title('Training and Testing Losses')
    plt.legend(['Train Loss', 'Test Loss'])
    plt.xlabel("epochs")
    plt.ylabel("loss")
    plt.xticks(range(0, 21, 4))
    plt.show()
